{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "daffc468",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from random import shuffle\n",
    "import numpy as np\n",
    "from numpy.random import randint\n",
    "from scipy.stats import ortho_group\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5522cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_len = 100\n",
    "samples_seq = [0] * (max_seq_len+1)\n",
    "seq_dict = {}\n",
    "encoding = \"dense_orthonormal\"\n",
    "x = list()\n",
    "y = list()\n",
    "y_mlp = list()\n",
    "raw_sequence = list()\n",
    "token_repeated = list()\n",
    "pos_first_token = list()\n",
    "sequence_len = list()\n",
    "eos_decoder = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dbb956a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_labels(sequence):\n",
    "    new_list = list()\n",
    "    seq_len = len(sequence)\n",
    "    label = [0]*(seq_len+1)\n",
    "    for num, letter in enumerate(sequence):\n",
    "        #print(num, letter)\n",
    "        if letter in new_list:\n",
    "            label[num] = 1\n",
    "\n",
    "        new_list.append(letter)\n",
    "\n",
    "    label[seq_len] = eos_decoder\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f37c3130",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_seq(seq_len, num_repeat, num_tokens_rep, positive, orthonormal_vectors):\n",
    "    \"\"\"\n",
    "    :param seq_len: length of the sequence\n",
    "    :param num_repeat: number of times the token needs to be repeated\n",
    "    :param repeat_dist: number of intervening tokens between reps\n",
    "    :param num_tokens_rep: number of tokens that are repeated in the seq\n",
    "    :return: sequence\n",
    "    \"\"\"\n",
    "    # repeat position - recency; random but be balanced accross dataset\n",
    "\n",
    "    seq_list = np.arange(0, max_seq_len)\n",
    "    shuffle(seq_list)\n",
    "    seq_list = seq_list[:seq_len]\n",
    "\n",
    "    if(positive):\n",
    "        # randomly generate first repeat position\n",
    "\n",
    "        first_pos = randint(0, seq_len-1)\n",
    "        # the repeated token is always at the end, there are no tokens after the\n",
    "        # repeated token\n",
    "        seq_list[first_pos] = seq_list[-1]\n",
    "        rep_token = seq_list[first_pos]\n",
    "\n",
    "    else:\n",
    "        # none of the tokens are repeating\n",
    "        rep_token = -1\n",
    "        first_pos = -1\n",
    "\n",
    "\n",
    "    return seq_list, rep_token, first_pos, seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e661c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_inputs(sequence, rep_token, first_token_pos, seq_len, positive, orthonormal_vectors):\n",
    "    seq_list = tuple(sequence)\n",
    "    if seq_list in seq_dict:\n",
    "        skipped = 1\n",
    "        return skipped\n",
    "    else:\n",
    "        skipped = 0\n",
    "        seq_dict[seq_list] = 1\n",
    "\n",
    "    # proceed to apend the sequence\n",
    "    sequence_one_hot = []\n",
    "    for token in sequence:\n",
    "\n",
    "        if(encoding == \"one_hot\"):\n",
    "            seq_token = [0] * (max_seq_len + 1)\n",
    "            seq_token[token] = 1\n",
    "        elif (encoding == \"dense_orthonormal\"):\n",
    "            seq_token = orthonormal_vectors[token]\n",
    "\n",
    "        sequence_one_hot.append(seq_token)\n",
    "    if(encoding == \"one_hot\"):\n",
    "        sequence_one_hot.append(eos_seq_ip)\n",
    "    elif(encoding == \"dense_orthonormal\"):\n",
    "        sequence_one_hot.append(orthonormal_vectors[-1])\n",
    "    x.append(sequence_one_hot)\n",
    "\n",
    "    raw_sequence.append(sequence)\n",
    "    label = generate_labels(sequence)\n",
    "    y.append(label)\n",
    "    y_mlp.append(positive)\n",
    "\n",
    "    token_repeated.append(rep_token)\n",
    "    pos_first_token.append(first_token_pos)\n",
    "    sequence_len.append(seq_len)\n",
    "\n",
    "    return skipped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9cdd8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset(max_seq_len=26, num_tokens_rep=1, num_instances_per_seq_len=5000):\n",
    "    \"\"\"\n",
    "    :param num_samples:\n",
    "    :param seq_len:\n",
    "    :param num_repeat:\n",
    "    :param repeat_dist:\n",
    "    :param num_tokens_rep:\n",
    "    :param max_seq_len:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    orthonormal_vectors = ortho_group.rvs(dim=(512)) # do not need orthonormal vectors for now\n",
    "    # min seq_len is always 2 as we do not consider 1 length sequence\n",
    "    min_seq_len = 2\n",
    "    num_repeat = 1\n",
    "\n",
    "    num_positive_examples = 0\n",
    "    num_negative_examples = 0\n",
    "\n",
    "    for seq_len in range(min_seq_len, max_seq_len+1):\n",
    "        #positive examples with repetion\n",
    "        #print(\"seq_len is\" + str(seq_len))\n",
    "        num_samples = min((max_seq_len*np.math.factorial(seq_len-1)), num_instances_per_seq_len)\n",
    "        # number of samples per sequence\n",
    "        samples_seq[seq_len] = num_samples*2\n",
    "        for sample in range(num_samples):\n",
    "            positive = 1\n",
    "            sequence, rep_token, first_token_pos, seq_len = generate_seq(seq_len,\n",
    "                                                                      num_repeat,\n",
    "                                                                      num_tokens_rep,\n",
    "                                                                       positive, orthonormal_vectors)\n",
    "            # while aggregating inputs do not add repeating samples\n",
    "            if sequence is not None:\n",
    "                skipped = aggregate_inputs(sequence, rep_token, first_token_pos, seq_len, positive, orthonormal_vectors)\n",
    "\n",
    "            if(skipped == 0):\n",
    "                num_positive_examples =num_positive_examples + 1\n",
    "                #negative samples, only when we have added a positive sample\n",
    "                positive = 0\n",
    "                sequence, rep_token, first_token_pos, seq_len = generate_seq(seq_len,\n",
    "                                                                      num_repeat,\n",
    "                                                                      num_tokens_rep,\n",
    "                                                                       positive, orthonormal_vectors)\n",
    "\n",
    "                skipped = aggregate_inputs(sequence, rep_token, first_token_pos, seq_len, positive, orthonormal_vectors)\n",
    "                if(skipped == 0):\n",
    "                    num_negative_examples = num_negative_examples + 1\n",
    "\n",
    "    print(\"Number of positive examples are: \" + str(num_positive_examples))\n",
    "    print(\"Number of negative examples are: \" + str(num_negative_examples))\n",
    "\n",
    "    return x, y, y_mlp, raw_sequence, token_repeated, pos_first_token, sequence_len, orthonormal_vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32d02f58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of positive examples are: 96860\n",
      "Number of negative examples are: 96860\n"
     ]
    }
   ],
   "source": [
    "x, y, y_mlp, raw_sequence, token_repeated, pos_first_token, sequence_len, orthonormal_vectors = generate_dataset(100,\n",
    "                                                                      1, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6079e493",
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate out the query and the rest of the seq and account for length\n",
    "raw_seq = [seq[:-1] for seq in raw_sequence]\n",
    "len_seq = [slen-1 for slen in sequence_len]\n",
    "query = [seq[-1] for seq in raw_sequence]\n",
    "# directly use token_repeated, pos_first_token, y_mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1e06ad24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([82,  1, 73])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_seq[1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "539cf859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe with these entries\n",
    "columns = ['sequence', 'query', 'sequence_len', 'first_rep_pos', 'label', 'token_repeated']\n",
    "syn_df = pd.DataFrame(columns = columns)\n",
    "syn_df['sequence'] = raw_seq\n",
    "syn_df['query'] = query\n",
    "syn_df['sequence_len'] = len_seq\n",
    "syn_df['first_rep_pos'] = pos_first_token\n",
    "syn_df['label'] = y_mlp\n",
    "syn_df['token_repeated'] = token_repeated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04d160b",
   "metadata": {},
   "outputs": [],
   "source": [
    "syn_df.to_pickle('synthetic_inputs.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "96dfbc02",
   "metadata": {},
   "outputs": [],
   "source": [
    "syn_df.to_json('synthetic_inputs.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f864fe0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
