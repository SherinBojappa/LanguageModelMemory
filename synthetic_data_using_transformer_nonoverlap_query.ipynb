{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "e7aece68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loaded backend module://matplotlib_inline.backend_inline version unknown.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "3f6986d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial6/Transformers_and_MHAttention.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "49342578",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams.update({'font.size': 18})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "f1685a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    " \n",
    "logging.basicConfig(filename = 'mem_with_transf_synth_train.log',\n",
    "                    level = logging.DEBUG,\n",
    "                    format = '%(asctime)s:%(levelname)s:%(name)s:%(message)s')\n",
    "logging.getLogger().addHandler(logging.StreamHandler())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "5ab172c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "7365253c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "8cbd0bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "956a1742",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "fa3984bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.utils.data import random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "9819926a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "ca85bf38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product(q, k, v, mask=None):\n",
    "    d_k = q.size()[-1]\n",
    "    attn_logits = torch.matmul(q, k.transpose(-2, -1))\n",
    "    attn_logits = attn_logits / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        attn_logits = attn_logits.masked_fill(mask == 0, -9e15)\n",
    "    attention = F.softmax(attn_logits, dim=-1)\n",
    "    values = torch.matmul(attention, v)\n",
    "    return values, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "86a4959b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiheadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0, \"Embedding dimension must be 0 modulo number of heads.\"\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "\n",
    "        # Stack all weight matrices 1...h together for efficiency\n",
    "        # Note that in many implementations you see \"bias=False\" which is optional\n",
    "        self.qkv_proj = nn.Linear(input_dim, 3*embed_dim)\n",
    "        self.o_proj = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "        self._reset_parameters()\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "        # Original Transformer initialization, see PyTorch documentation\n",
    "        nn.init.xavier_uniform_(self.qkv_proj.weight)\n",
    "        self.qkv_proj.bias.data.fill_(0)\n",
    "        nn.init.xavier_uniform_(self.o_proj.weight)\n",
    "        self.o_proj.bias.data.fill_(0)\n",
    "\n",
    "    def forward(self, x, mask=None, return_attention=False):\n",
    "        batch_size, seq_length, embed_dim = x.size()\n",
    "        qkv = self.qkv_proj(x)\n",
    "\n",
    "        # Separate Q, K, V from linear output\n",
    "        qkv = qkv.reshape(batch_size, seq_length, self.num_heads, 3*self.head_dim)\n",
    "        qkv = qkv.permute(0, 2, 1, 3) # [Batch, Head, SeqLen, Dims]\n",
    "        q, k, v = qkv.chunk(3, dim=-1)\n",
    "\n",
    "        # Determine value outputs\n",
    "        values, attention = scaled_dot_product(q, k, v, mask=mask)\n",
    "        values = values.permute(0, 2, 1, 3) # [Batch, SeqLen, Head, Dims]\n",
    "        values = values.reshape(batch_size, seq_length, embed_dim)\n",
    "        o = self.o_proj(values)\n",
    "\n",
    "        if return_attention:\n",
    "            return o, attention\n",
    "        else:\n",
    "            return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "962b1a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, num_heads, dim_feedforward, dropout=0.0):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            input_dim - Dimensionality of the input\n",
    "            num_heads - Number of heads to use in the attention block\n",
    "            dim_feedforward - Dimensionality of the hidden layer in the MLP\n",
    "            dropout - Dropout probability to use in the dropout layers\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Attention layer\n",
    "        self.self_attn = MultiheadAttention(input_dim, input_dim, num_heads)\n",
    "\n",
    "        # Two-layer MLP\n",
    "        self.linear_net = nn.Sequential(\n",
    "            nn.Linear(input_dim, dim_feedforward),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(dim_feedforward, input_dim)\n",
    "        )\n",
    "\n",
    "        # Layers to apply in between the main layers\n",
    "        self.norm1 = nn.LayerNorm(input_dim)\n",
    "        self.norm2 = nn.LayerNorm(input_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # Attention part\n",
    "        attn_out = self.self_attn(x, mask=mask)\n",
    "        x = x + self.dropout(attn_out)\n",
    "        x = self.norm1(x)\n",
    "\n",
    "        # MLP part\n",
    "        linear_out = self.linear_net(x)\n",
    "        x = x + self.dropout(linear_out)\n",
    "        x = self.norm2(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "1d10f893",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "\n",
    "    def __init__(self, num_layers, **block_args):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([EncoderBlock(**block_args) for _ in range(num_layers)])\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        for l in self.layers:\n",
    "            x = l(x, mask=mask)\n",
    "        return x\n",
    "\n",
    "    def get_attention_maps(self, x, mask=None):\n",
    "        attention_maps = []\n",
    "        for l in self.layers:\n",
    "            _, attn_map = l.self_attn(x, mask=mask, return_attention=True)\n",
    "            attention_maps.append(attn_map)\n",
    "            x = l(x)\n",
    "        return attention_maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "ec06c8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        \"\"\"\n",
    "        Inputs\n",
    "            d_model - Hidden dimensionality of the input.\n",
    "            max_len - Maximum length of a sequence to expect.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Create matrix of [SeqLen, HiddenDim] representing the positional encoding for max_len inputs\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "\n",
    "        # register_buffer => Tensor which is not a parameter, but should be part of the modules state.\n",
    "        # Used for tensors that need to be on the same device as the module.\n",
    "        # persistent=False tells PyTorch to not add the buffer to the state dict (e.g. when we save the model)\n",
    "        self.register_buffer('pe', pe, persistent=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "066d5aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CosineWarmupScheduler(optim.lr_scheduler._LRScheduler):\n",
    "\n",
    "    def __init__(self, optimizer, warmup, max_iters):\n",
    "        self.warmup = warmup\n",
    "        self.max_num_iters = max_iters\n",
    "        super().__init__(optimizer)\n",
    "\n",
    "    def get_lr(self):\n",
    "        lr_factor = self.get_lr_factor(epoch=self.last_epoch)\n",
    "        return [base_lr * lr_factor for base_lr in self.base_lrs]\n",
    "\n",
    "    def get_lr_factor(self, epoch):\n",
    "        lr_factor = 0.5 * (1 + np.cos(np.pi * epoch / self.max_num_iters))\n",
    "        if epoch <= self.warmup:\n",
    "            lr_factor *= epoch * 1.0 / self.warmup\n",
    "        return lr_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "48d4f81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_context_query = np.load('train_context_query.npy', allow_pickle=True)\n",
    "val_context_query = np.load('val_context_query.npy', allow_pickle=True)\n",
    "test_context_query = np.load('test_context_query.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "dbe1caeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train= pd.read_json('synthetic_train.json')\n",
    "df_val = pd.read_json('synthetic_val.json')\n",
    "df_test = pd.read_json('synthetic_test.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "0db0f8bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of the synthetic train dataset is 94919\n",
      "The size of the synthetic val dataset is 40680\n",
      "The size of the synthetic test dataset is 58115\n"
     ]
    }
   ],
   "source": [
    "print(\"The size of the synthetic train dataset is {}\".format(len(df_train)))\n",
    "print(\"The size of the synthetic val dataset is {}\".format(len(df_val)))\n",
    "print(\"The size of the synthetic test dataset is {}\".format(len(df_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "a22df21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.concat([df_train, df_val, df_test])\n",
    "all_context_query = np.hstack([train_context_query, val_context_query, test_context_query])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "fe7ba3ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>seq_len</th>\n",
       "      <th>seq</th>\n",
       "      <th>rep_token_first_pos</th>\n",
       "      <th>query_token</th>\n",
       "      <th>target_val</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>44654</th>\n",
       "      <td>44654</td>\n",
       "      <td>25</td>\n",
       "      <td>[81  9 32 70 73 68 19 85  1 30 15 45 82 64 38 ...</td>\n",
       "      <td>12</td>\n",
       "      <td>82</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85524</th>\n",
       "      <td>85524</td>\n",
       "      <td>45</td>\n",
       "      <td>[54 48 92 66 35 94 53 95 24 49  4  5 72 62 15 ...</td>\n",
       "      <td>20</td>\n",
       "      <td>85</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9508</th>\n",
       "      <td>9508</td>\n",
       "      <td>7</td>\n",
       "      <td>[54 30  8  1 19 71 73]</td>\n",
       "      <td>0</td>\n",
       "      <td>54</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21160</th>\n",
       "      <td>21160</td>\n",
       "      <td>13</td>\n",
       "      <td>[68  5 84 25 39 12 35 15 97 31 64 88 38]</td>\n",
       "      <td>8</td>\n",
       "      <td>97</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193670</th>\n",
       "      <td>193670</td>\n",
       "      <td>99</td>\n",
       "      <td>[44 50 31 65 97 60 95 40 42 30 14 62 10 33 72 ...</td>\n",
       "      <td>62</td>\n",
       "      <td>93</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         index  seq_len                                                seq  \\\n",
       "44654    44654       25  [81  9 32 70 73 68 19 85  1 30 15 45 82 64 38 ...   \n",
       "85524    85524       45  [54 48 92 66 35 94 53 95 24 49  4  5 72 62 15 ...   \n",
       "9508      9508        7                             [54 30  8  1 19 71 73]   \n",
       "21160    21160       13           [68  5 84 25 39 12 35 15 97 31 64 88 38]   \n",
       "193670  193670       99  [44 50 31 65 97 60 95 40 42 30 14 62 10 33 72 ...   \n",
       "\n",
       "        rep_token_first_pos  query_token  target_val  \n",
       "44654                    12           82           1  \n",
       "85524                    20           85           1  \n",
       "9508                      0           54           1  \n",
       "21160                     8           97           1  \n",
       "193670                   62           93           1  "
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "3156e81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_val_query, test_query = train_test_split(df_all[\"query_token\"].unique().tolist(), test_size=0.3, random_state=2, shuffle=True)\n",
    "train_query, val_query = train_test_split(train_val_query, test_size=0.3, random_state=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "ebaa7093",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids = df_all[\"query_token\"].isin(train_query)\n",
    "val_ids = df_all[\"query_token\"].isin(val_query)\n",
    "test_ids = df_all[\"query_token\"].isin(test_query)\n",
    "\n",
    "new_df_train = df_all[train_ids]\n",
    "new_df_val = df_all[val_ids]\n",
    "new_df_test = df_all[test_ids]\n",
    "\n",
    "new_context_query_train = all_context_query[train_ids]\n",
    "new_context_query_val = all_context_query[val_ids]\n",
    "new_context_query_test = all_context_query[test_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "19b00109",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of the synthetic train dataset is 94919\n",
      "The size of the synthetic val dataset is 40680\n",
      "The size of the synthetic test dataset is 58115\n"
     ]
    }
   ],
   "source": [
    "print(\"The size of the synthetic train dataset is {}\".format(len(df_train)))\n",
    "print(\"The size of the synthetic val dataset is {}\".format(len(df_val)))\n",
    "print(\"The size of the synthetic test dataset is {}\".format(len(df_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "a295f75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "INP_PATH = '/data/sherin/'\n",
    "orth_vectors = np.load(INP_PATH + 'orthonormal_vectors_512.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "fc30cffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#orth_vectors = np.random.randn(512, 512)\n",
    "#orth_vectors = np.random.normal(0,0.01, (512, 75))\n",
    "orth_vectors = np.random.rand(512, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "579e483c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tensors(inp, Orth):\n",
    "    C = np.zeros((len(inp), 99, 512))\n",
    "    Q = np.zeros((len(inp), 512))\n",
    "    for idx, c_q in enumerate(inp):\n",
    "        #print(c_q.shape)\n",
    "        Q[idx] = Orth[c_q[-1]]\n",
    "        C[idx,99 - (len(c_q)-1):,:] = Orth[c_q[:-1]]\n",
    "    return C, Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "1886507a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_C, train_Q = get_tensors(new_context_query_train, orth_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "673ef7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_C, val_Q = get_tensors(new_context_query_val, orth_vectors)\n",
    "test_C, test_Q = get_tensors(new_context_query_test, orth_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "be409db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label = new_df_train['target_val'].values\n",
    "val_label = new_df_val['target_val'].values\n",
    "test_label = new_df_test['target_val'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "97b3eb92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_loader(context_reps, query_reps, label, batch_size, shuffle):\n",
    "    data_set = TensorDataset(context_reps, query_reps, label)\n",
    "    loader = DataLoader(data_set, batch_size=batch_size, shuffle=shuffle)\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "c3f20e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "34c5242f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = get_data_loader(torch.tensor(train_C).float(), torch.tensor(train_Q).float(),\n",
    "                               torch.tensor(train_label), batch_size, shuffle=True)\n",
    "val_loader = get_data_loader(torch.tensor(val_C).float(), torch.tensor(val_Q).float(),\n",
    "                             torch.tensor(val_label), batch_size, shuffle=False)\n",
    "test_loader = get_data_loader(torch.tensor(test_C).float(), torch.tensor(test_Q).float(),\n",
    "                              torch.tensor(test_label), batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "ac9e0227",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nclass TransformerPredictor(nn.Module):\\n\\n    def __init__(self, input_dim=512, model_dim=32,\\n                 num_heads=1, num_layers=1,\\n                 dropout=0.0, input_dropout=0.0):\\n\\n        super().__init__()\\n\\n        # Input dim -> Model dim\\n        self.input_net = nn.Sequential(\\n            #nn.Dropout(input_dropout),\\n            nn.Linear(input_dim, model_dim)\\n        )\\n        # Positional encoding for sequences\\n        self.positional_encoding = PositionalEncoding(d_model=model_dim, max_len=100)\\n        # Transformer\\n        self.transformer = TransformerEncoder(num_layers=num_layers,\\n                                              input_dim=model_dim,\\n                                              dim_feedforward=2*model_dim,\\n                                              num_heads=num_heads,\\n                                              dropout=dropout)\\n\\n    def forward(self, x, y):\\n        x = self.input_net(x)\\n        y = self.input_net(y)\\n        #print(x.shape)\\n        #print(y.shape)\\n        x = self.positional_encoding(x)\\n        #y = self.positional_encoding(y.unsqueeze(1))\\n        x = self.transformer(x)\\n        #print(x.shape)\\n        #print(y.shape)\\n        #op = torch.sum(x[:,-1,:]*y.squeeze(1), dim=1)\\n        #op = torch.sum(torch.sum(x, dim=1)*y.squeeze(1), dim=1)\\n        op = torch.sum(torch.sum(x, dim=1)*y, dim=1)\\n        #op = torch.sum(x[:,-1,:]*y, dim=1)\\n        return op\\n'"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "class TransformerPredictor(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim=512, model_dim=32,\n",
    "                 num_heads=1, num_layers=1,\n",
    "                 dropout=0.0, input_dropout=0.0):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # Input dim -> Model dim\n",
    "        self.input_net = nn.Sequential(\n",
    "            #nn.Dropout(input_dropout),\n",
    "            nn.Linear(input_dim, model_dim)\n",
    "        )\n",
    "        # Positional encoding for sequences\n",
    "        self.positional_encoding = PositionalEncoding(d_model=model_dim, max_len=100)\n",
    "        # Transformer\n",
    "        self.transformer = TransformerEncoder(num_layers=num_layers,\n",
    "                                              input_dim=model_dim,\n",
    "                                              dim_feedforward=2*model_dim,\n",
    "                                              num_heads=num_heads,\n",
    "                                              dropout=dropout)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        x = self.input_net(x)\n",
    "        y = self.input_net(y)\n",
    "        #print(x.shape)\n",
    "        #print(y.shape)\n",
    "        x = self.positional_encoding(x)\n",
    "        #y = self.positional_encoding(y.unsqueeze(1))\n",
    "        x = self.transformer(x)\n",
    "        #print(x.shape)\n",
    "        #print(y.shape)\n",
    "        #op = torch.sum(x[:,-1,:]*y.squeeze(1), dim=1)\n",
    "        #op = torch.sum(torch.sum(x, dim=1)*y.squeeze(1), dim=1)\n",
    "        op = torch.sum(torch.sum(x, dim=1)*y, dim=1)\n",
    "        #op = torch.sum(x[:,-1,:]*y, dim=1)\n",
    "        return op\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "092f4791",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TransformerPredictor(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim=512, model_dim=32,\n",
    "                 num_heads=1, num_layers=1,\n",
    "                 dropout=0.0, input_dropout=0.0):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # Input dim -> Model dim\n",
    "        self.input_net = nn.Sequential(\n",
    "            #nn.Dropout(input_dropout),\n",
    "            nn.Linear(input_dim, model_dim)\n",
    "        )\n",
    "        self.output_net = nn.Sequential(\n",
    "            #nn.Dropout(input_dropout),\n",
    "            nn.Linear(model_dim, input_dim)\n",
    "        )\n",
    "        # Positional encoding for sequences\n",
    "        #self.positional_encoding = PositionalEncoding(d_model=model_dim, max_len=100)\n",
    "        # Transformer\n",
    "        self.transformer = TransformerEncoder(num_layers=num_layers,\n",
    "                                              input_dim=model_dim,\n",
    "                                              dim_feedforward=2*model_dim,\n",
    "                                              num_heads=num_heads,\n",
    "                                              dropout=dropout)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        x = self.input_net(x)\n",
    "        #y = self.input_net(y)\n",
    "        #print(x.shape)\n",
    "        #print(y.shape)\n",
    "        #x = self.positional_encoding(x)\n",
    "        #y = self.positional_encoding(y.unsqueeze(1))\n",
    "        x = self.transformer(x)\n",
    "        #print(x.shape)\n",
    "        #print(y.shape)\n",
    "        #op = torch.sum(x[:,-1,:]*y.squeeze(1), dim=1)\n",
    "        #op = torch.sum(torch.sum(x, dim=1)*y.squeeze(1), dim=1)\n",
    "        x = self.output_net(torch.sum(x, dim=1))\n",
    "        op = torch.sum(x*y, dim=1)\n",
    "        #op = torch.sum(x[:,-1,:]*y, dim=1)\n",
    "        return op\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742cd0be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "6a373eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TransformerPredictor(num_layers=2, num_heads=2, model_dim=240).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "7f353116",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerPredictor(\n",
       "  (input_net): Sequential(\n",
       "    (0): Linear(in_features=512, out_features=240, bias=True)\n",
       "  )\n",
       "  (output_net): Sequential(\n",
       "    (0): Linear(in_features=240, out_features=512, bias=True)\n",
       "  )\n",
       "  (transformer): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): EncoderBlock(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (qkv_proj): Linear(in_features=240, out_features=720, bias=True)\n",
       "          (o_proj): Linear(in_features=240, out_features=240, bias=True)\n",
       "        )\n",
       "        (linear_net): Sequential(\n",
       "          (0): Linear(in_features=240, out_features=480, bias=True)\n",
       "          (1): Dropout(p=0.0, inplace=False)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Linear(in_features=480, out_features=240, bias=True)\n",
       "        )\n",
       "        (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (1): EncoderBlock(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (qkv_proj): Linear(in_features=240, out_features=720, bias=True)\n",
       "          (o_proj): Linear(in_features=240, out_features=240, bias=True)\n",
       "        )\n",
       "        (linear_net): Sequential(\n",
       "          (0): Linear(in_features=240, out_features=480, bias=True)\n",
       "          (1): Dropout(p=0.0, inplace=False)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Linear(in_features=480, out_features=240, bias=True)\n",
       "        )\n",
       "        (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "745a47f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "params = sum([np.prod(p.size()) for p in model_parameters])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "be4bebf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1173392\n"
     ]
    }
   ],
   "source": [
    "print(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "d51916fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "#scheduler = StepLR(optimizer, step_size=5, gamma=0.8)\n",
    "scheduler = CosineWarmupScheduler(optimizer, warmup=50,\n",
    "                                             max_iters=num_epochs*len(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "eca5300e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checkpoint_path = '/data/sherin/checkpoint_lm/chkpt_synth_tran_posenc_steplr_recall_best.pt.tar'\n",
    "checkpoint_path = '/data/sherin/checkpoint_lm/chkpt_synth_tran_dim_240_steplr_recall_best.pt.tar'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "b4e8fc3c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2955/2955 [00:56<00:00, 52.57it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1281/1281 [00:22<00:00, 58.22it/s]\n",
      "saving best model\n",
      "saving best model\n",
      "curr_lr: 0.0009997532801828658\n",
      "curr_lr: 0.0009997532801828658\n",
      "[1] Training loss: 3.334 Training accuracy : 0.543\n",
      "[1] Training loss: 3.334 Training accuracy : 0.543\n",
      "[1] Validation loss: 3.607 Validation accuracy : 0.498\n",
      "[1] Validation loss: 3.607 Validation accuracy : 0.498\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2955/2955 [01:25<00:00, 34.70it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1281/1281 [00:14<00:00, 89.55it/s]\n",
      "saving best model\n",
      "saving best model\n",
      "curr_lr: 0.0009990133642141358\n",
      "curr_lr: 0.0009990133642141358\n",
      "[2] Training loss: 0.736 Training accuracy : 0.573\n",
      "[2] Training loss: 0.736 Training accuracy : 0.573\n",
      "[2] Validation loss: 0.807 Validation accuracy : 0.504\n",
      "[2] Validation loss: 0.807 Validation accuracy : 0.504\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2955/2955 [00:55<00:00, 53.39it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1281/1281 [00:14<00:00, 87.60it/s]\n",
      "not saving the model\n",
      "not saving the model\n",
      "curr_lr: 0.00099778098230154\n",
      "curr_lr: 0.00099778098230154\n",
      "[3] Training loss: 0.712 Training accuracy : 0.538\n",
      "[3] Training loss: 0.712 Training accuracy : 0.538\n",
      "[3] Validation loss: 0.721 Validation accuracy : 0.502\n",
      "[3] Validation loss: 0.721 Validation accuracy : 0.502\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2955/2955 [01:00<00:00, 48.86it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1281/1281 [00:13<00:00, 97.21it/s]\n",
      "not saving the model\n",
      "not saving the model\n",
      "curr_lr: 0.000996057350657239\n",
      "curr_lr: 0.000996057350657239\n",
      "[4] Training loss: 0.713 Training accuracy : 0.524\n",
      "[4] Training loss: 0.713 Training accuracy : 0.524\n",
      "[4] Validation loss: 0.699 Validation accuracy : 0.498\n",
      "[4] Validation loss: 0.699 Validation accuracy : 0.498\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2955/2955 [00:56<00:00, 52.22it/s]\n",
      " 54%|██████████████████████████████████████████████████████████████████████████████████████▎                                                                         | 691/1281 [00:07<00:06, 98.08it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_518802/11508654.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mtest_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0mtest_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_count\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/tf14/lib/python3.7/site-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1194\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1195\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1196\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1197\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/tf14/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/tf14/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/tf14/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/envs/tf14/lib/python3.7/site-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'each element in list of batch should be of equal size'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0mtransposed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefault_collate_err_msg_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/tf14/lib/python3.7/site-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'each element in list of batch should be of equal size'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0mtransposed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefault_collate_err_msg_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/tf14/lib/python3.7/site-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new_shared\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__module__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'numpy'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'str_'\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'string_'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epoch_loss_list = []\n",
    "accuracy_list = []\n",
    "val_loss_list = []\n",
    "val_acc_list = []\n",
    "valid_acc_max = 0 \n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_count = 0\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    accuracy = 0.0\n",
    "    \n",
    "\n",
    "    for context, query, labels in tqdm(train_loader):\n",
    "        train_count = train_count+1\n",
    "        context = context.to(device)\n",
    "        query = query.to(device)    \n",
    "        target = labels.to(device)\n",
    "        label = labels.float().to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(context, query)\n",
    "        loss = criterion(outputs, label)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        accuracy += torchmetrics.functional.accuracy(outputs, target, threshold=0.5).item()\n",
    "        \n",
    "        scheduler.step()\n",
    "\n",
    "    \n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_acc = 0.0\n",
    "    \n",
    "    test_count = 0\n",
    "    for context, query, labels in tqdm(val_loader):\n",
    "        test_count = test_count + 1\n",
    "        context = context.to(device)\n",
    "        query = query.to(device)\n",
    "        target = labels.to(device)\n",
    "        label = labels.float().to(device)\n",
    "        \n",
    "        outputs = model(context, query)\n",
    "        loss = criterion(outputs, label)\n",
    "        val_loss += loss.item()\n",
    "        val_acc += torchmetrics.functional.accuracy(outputs, target, threshold=0.5).item()\n",
    "        \n",
    "    accuracy = accuracy / train_count\n",
    "    epoch_loss = epoch_loss / train_count\n",
    "    val_loss = val_loss / test_count\n",
    "    val_acc = val_acc / test_count\n",
    "    \n",
    "    if val_acc > valid_acc_max:\n",
    "        logging.info(\"saving best model\")\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': val_loss,\n",
    "            'accuracy': val_acc,\n",
    "            }, checkpoint_path)\n",
    "        valid_acc_max = val_acc\n",
    "    else:\n",
    "        logging.info(\"not saving the model\")\n",
    "    \n",
    "    curr_lr = optimizer.param_groups[0]['lr']\n",
    "    logging.info(f'curr_lr: {curr_lr}')\n",
    "    logging.info(f'[{epoch + 1}] Training loss: {epoch_loss:.3f} Training accuracy : {accuracy:.3f}')\n",
    "    logging.info(f'[{epoch + 1}] Validation loss: {val_loss:.3f} Validation accuracy : {val_acc:.3f}')\n",
    "    epoch_loss_list.append(epoch_loss)\n",
    "    accuracy_list.append(accuracy)\n",
    "    val_loss_list.append(val_loss)\n",
    "    val_acc_list.append(val_acc)\n",
    "    \n",
    "    # scheduler.step()\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "a2842d1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished\n"
     ]
    }
   ],
   "source": [
    "print(\"finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "62c46c69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerPredictor(\n",
       "  (input_net): Sequential(\n",
       "    (0): Linear(in_features=512, out_features=240, bias=True)\n",
       "  )\n",
       "  (output_net): Sequential(\n",
       "    (0): Linear(in_features=240, out_features=512, bias=True)\n",
       "  )\n",
       "  (transformer): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): EncoderBlock(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (qkv_proj): Linear(in_features=240, out_features=720, bias=True)\n",
       "          (o_proj): Linear(in_features=240, out_features=240, bias=True)\n",
       "        )\n",
       "        (linear_net): Sequential(\n",
       "          (0): Linear(in_features=240, out_features=480, bias=True)\n",
       "          (1): Dropout(p=0.0, inplace=False)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Linear(in_features=480, out_features=240, bias=True)\n",
       "        )\n",
       "        (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (1): EncoderBlock(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (qkv_proj): Linear(in_features=240, out_features=720, bias=True)\n",
       "          (o_proj): Linear(in_features=240, out_features=240, bias=True)\n",
       "        )\n",
       "        (linear_net): Sequential(\n",
       "          (0): Linear(in_features=240, out_features=480, bias=True)\n",
       "          (1): Dropout(p=0.0, inplace=False)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Linear(in_features=480, out_features=240, bias=True)\n",
       "        )\n",
       "        (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH = checkpoint_path\n",
    "checkpoint = torch.load(PATH)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "epoch = checkpoint['epoch']\n",
    "loss = checkpoint['loss']\n",
    "acc = checkpoint['accuracy']\n",
    "\n",
    "# inferece\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "a9c40b9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5011953551912568\n"
     ]
    }
   ],
   "source": [
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "5dcadc81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1818/1818 [00:28<00:00, 62.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test accuracy is 0.501117299229923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_count = 0\n",
    "output_logits_trans = []\n",
    "test_acc = 0\n",
    "for context, query, labels in tqdm(test_loader):\n",
    "    test_count = test_count + 1\n",
    "    context = context.to(device)\n",
    "    query = query.to(device)\n",
    "    target = labels.to(device)\n",
    "    label = labels.float().to(device)\n",
    "\n",
    "    outputs = model(context, query)\n",
    "    output_logits = outputs.detach().cpu().numpy()\n",
    "    test_acc += torchmetrics.functional.accuracy(outputs, target, threshold=0.5).item()\n",
    "    output_logits_trans.append(output_logits)\n",
    "\n",
    "accuracy = test_acc/test_count\n",
    "print(\"The test accuracy is {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "bf8a11f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_output_logits_trans = np.hstack(output_logits_trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "225304d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_pred = torch.sigmoid(torch.tensor(test_output_logits_trans))\n",
    "trans_pred_label = 1.0 * (trans_pred > 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "cfed4fc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18624518701870188"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(1.0 * (trans_pred_label.cpu().numpy()==1))/len(trans_pred_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "9e4b985b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5011344884488449"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(1.0 * (test_label==1))/len(test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "7440ab07",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(trans_pred, 'new_trans_pred.pt')\n",
    "torch.save(trans_pred_label, 'new_trans_pred_label.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a87edeb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
